"""
This script downloads a list of GeoJSON file entries from NOAA SWPC's experimental product.
It has four modes:
  1. Normal CZML mode:
     - Using a user-specified target date/time, it finds the file whose time_tag is closest to that target,
       then processes that file and up to NUM_URLS files sequentially.
     - For each file, it downloads the GeoJSON, extracts each feature’s coordinates and hmF2 value to create a vertical polyline,
       and adds a "show" block so that the polyline is visible from the file’s time_tag until 10 minutes later.
     - A document CZML packet is created with a clock entry covering the overall interval.
  2. -alldb mode:
     - All files in the listing are processed.
     - For each file and for each feature in that file, a CSV row is written (and the same data inserted into a SQLite database)
       containing the following columns:
         timestamp, longitude, latitude, hmF2, NmF2, quality_flag
       where uid is automatically generated by SQLite.
  3. -updatedb <timestamp> mode:
     - The script accepts a timestamp (formatted as 2025-02-03T00:45:00Z) and processes all geojson entries
       whose time_tag is later than the provided timestamp, appending new records into glotec.db.
  4. -appnd_ncd <netcdf_link> mode:
     - The script downloads a netCDF4 file from the provided URL. The file is expected to have variables:
         time (shape: 144),
         latitude (shape: 72),
         longitude (shape: 72),
         hmF2 (shape: (144, 72, 72)), and
         quality_flag (shape: (144, 72, 72)).
       It loops through the time dimension and for each grid cell creates a record with columns:
         timestamp, longitude, latitude, hmF2, NmF2, quality_flag,
       and appends each record to the existing glotec.db database.
  5. -nmpatch <start_timestamp> <end_timestamp> mode:
     - The script accepts two timestamps (formatted as 2025-02-03T00:45:00Z) defining a date range.
       It deletes all rows in the glotec table whose timestamp is within that range, and then replaces them
       by processing the corresponding files from the NOAA listing (including the related NmF2 values).
  6. -laglotec mode:
     - The script retrieves only the most recent glotec entry from the listing and creates a new glotec database
       containing only the records from that most recent GeoJSON file.
"""

import sys
import json
import requests
import csv
import sqlite3
import math
import tempfile
import numpy as np
from datetime import datetime, timedelta
from netCDF4 import Dataset, num2date

# -------------------------
# User-adjustable parameters:
NUM_URLS = 30  # maximum number of URLs to process (for CZML mode)

# Specify the target date/time (in UTC) to start with (for CZML mode).
target_year = 2025
target_month = 1
target_day = 2
target_hour = 4
target_minute = 0
target_dt = datetime(target_year, target_month, target_day, target_hour, target_minute)

# Base URL to prepend to relative URLs from the file list
BASE_URL = "https://services.swpc.noaa.gov/"

# URL to the JSON file that lists the GeoJSON file entries
LISTING_URL = "https://services.swpc.noaa.gov/products/glotec/geojson_2d_urt.json"


# -------------------------
# Function for "all database dump" mode.
def dump_all_data():
    print("Running in -alldb mode: dumping all data into CSV and SQLite database.")
    try:
        r = requests.get(LISTING_URL)
        r.raise_for_status()
        full_listing = r.json()
    except Exception as e:
        print(f"Error downloading listing JSON: {e}")
        sys.exit(1)

    if not full_listing:
        print("No entries found in the listing.")
        sys.exit(1)

    csv_filename = "glotec.csv"
    db_filename = "glotec.db"
    csv_fieldnames = ["timestamp", "tec", "anomaly", "longitude", "latitude", "hmF2", "NmF2", "quality_flag", "uid"]

    try:
        csv_file = open(csv_filename, "w", newline='')
        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_fieldnames)
        # Uncomment the next line if you want to write a header:
        # csv_writer.writeheader()
    except Exception as e:
        print(f"Error opening CSV file {csv_filename}: {e}")
        sys.exit(1)

    try:
        conn = sqlite3.connect(db_filename)
        cur = conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS glotec (
                uid INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                longitude REAL,
                latitude REAL,
                hmF2 INTEGER,
                NmF2 REAL,
                quality_flag TEXT
            )
        """)
        conn.commit()
    except Exception as e:
        print(f"Error setting up SQLite database {db_filename}: {e}")
        sys.exit(1)

    total_rows = 0

    for entry in full_listing:
        try:
            file_time = datetime.strptime(entry["time_tag"], "%Y-%m-%dT%H:%M:%SZ")
        except Exception as e:
            print(f"Error parsing time_tag {entry.get('time_tag')}: {e}")
            continue

        timestamp_str = entry["time_tag"]

        rel_url = entry.get("url")
        if not rel_url:
            print("Skipping an entry because it has no 'url' field.")
            continue
        full_url = BASE_URL + rel_url
        print("Processing file: " + full_url)

        try:
            r = requests.get(full_url)
            r.raise_for_status()
            geojson_data = r.json()
        except Exception as e:
            print(f"Error downloading GeoJSON from {full_url}: {e}")
            continue

        features = geojson_data.get("features", [])
        for i, feature in enumerate(features):
            properties = feature.get("properties", {})
            geometry = feature.get("geometry", {})

            if geometry.get("type") != "Point":
                print(f"Skipping feature {i} in file {rel_url}: geometry type is not 'Point'.")
                continue

            coords = geometry.get("coordinates", [])
            if len(coords) < 2:
                print(f"Skipping feature {i} in file {rel_url}: insufficient coordinate data.")
                continue

            longitude, latitude = coords[0], coords[1]

            hmF2 = properties.get("hmF2")
            NmF2 = properties.get("NmF2")
            quality_flag = properties.get("quality_flag")

            try:
                hmF2 = math.trunc(float(hmF2))
            except Exception:
                print("hmF2 failed " + str(hmF2))
                hmF2 = None

            try:
                NmF2 = float(NmF2) if NmF2 is not None else None
            except Exception:
                print("NmF2 conversion failed " + str(NmF2))
                NmF2 = None

            try:
                cur.execute("""
                    INSERT INTO glotec (timestamp, longitude, latitude, hmF2, NmF2, quality_flag)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (timestamp_str, longitude, latitude, hmF2, NmF2, quality_flag))
                uid = cur.lastrowid
            except Exception as e:
                print(f"Error inserting row into database: {e}")
                continue

            total_rows += 1

    conn.commit()
    conn.close()
    csv_file.close()

    print(f"Dumped data for {total_rows} features into {csv_filename} and {db_filename}.")


# -------------------------
# Function for updating the database from geojson entries after a given timestamp.
# -------------------------
# Function for updating the database from geojson entries after a given timestamp.
def update_db(timestamp_input, timestamp_end):
    """
    Accepts a timestamp string in ISO 8601 format (e.g., 2025-02-03T00:45:00Z). It fetches all
    entries from the listing whose time_tag is later than the provided timestamp and appends
    their data to the (possibly newly-created) glotec.db database.
    """
    try:
        update_dt = datetime.strptime(timestamp_input, "%Y-%m-%dT%H:%M:%SZ")
        update_end = datetime.strptime(timestamp_end, "%Y-%m-%dT%H:%M:%SZ")
    except Exception as e:
        print(f"Error parsing provided timestamp {timestamp_input}: {e}")
        sys.exit(1)

    print("about to update")
    print(f"Updating database with entries later than {timestamp_input}.")

    # Get the listing
    try:
        r = requests.get(LISTING_URL)
        r.raise_for_status()
        full_listing = r.json()
    except Exception as e:
        print(f"Error downloading listing JSON: {e}")
        sys.exit(1)

    # Filter entries by time range
    filtered_entries = []
    for entry in full_listing:
        try:
            file_dt = datetime.strptime(entry["time_tag"], "%Y-%m-%dT%H:%M:%SZ")
        except Exception as e:
            print(f"Error parsing time_tag {entry.get('time_tag')}: {e}")
            continue

        # same logic you had before
        if file_dt > update_dt and update_dt == update_end:
            filtered_entries.append(entry)
        elif file_dt > update_dt and file_dt < update_end:
            filtered_entries.append(entry)

    if not filtered_entries:
        print("No new entries found after the provided timestamp.")
        sys.exit(0)

    print(f"Found {len(filtered_entries)} new entries to process.")

    db_filename = "glotec.db"

    # OPEN (creates file if missing) AND ENSURE TABLE EXISTS — matches other modes
    try:
        conn = sqlite3.connect(db_filename)
        cur = conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS glotec (
                uid INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                longitude REAL,
                latitude REAL,
                hmF2 INTEGER,
                NmF2 REAL,
                quality_flag TEXT
            )
        """)
        conn.commit()
    except Exception as e:
        print(f"Error setting up SQLite database {db_filename}: {e}")
        sys.exit(1)

    total_new_rows = 0

    for entry in filtered_entries:
        timestamp_str = entry["time_tag"]

        rel_url = entry.get("url")
        if not rel_url:
            print("Skipping an entry because it has no 'url' field.")
            continue
        full_url = BASE_URL + rel_url
        print("Processing file: " + full_url)

        try:
            r = requests.get(full_url)
            r.raise_for_status()
            geojson_data = r.json()
        except Exception as e:
            print(f"Error downloading GeoJSON from {full_url}: {e}")
            continue

        features = geojson_data.get("features", [])
        for i, feature in enumerate(features):
            properties = feature.get("properties", {})
            geometry = feature.get("geometry", {})

            if geometry.get("type") != "Point":
                print(f"Skipping feature {i} in file {rel_url}: geometry type is not 'Point'.")
                continue

            coords = geometry.get("coordinates", [])
            if len(coords) < 2:
                print(f"Skipping feature {i} in file {rel_url}: insufficient coordinate data.")
                continue

            longitude, latitude = coords[0], coords[1]

            hmF2 = properties.get("hmF2")
            NmF2 = properties.get("NmF2")
            quality_flag = properties.get("quality_flag")

            try:
                hmF2 = math.trunc(float(hmF2))
            except Exception:
                print("hmF2 failed " + str(hmF2))
                hmF2 = None

            try:
                NmF2 = float(NmF2) if NmF2 is not None else None
            except Exception:
                print("NmF2 conversion failed " + str(NmF2))
                NmF2 = None

            try:
                cur.execute("""
                    INSERT INTO glotec (timestamp, longitude, latitude, hmF2, NmF2, quality_flag)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (timestamp_str, longitude, latitude, hmF2, NmF2, quality_flag))
                total_new_rows += 1
            except Exception as e:
                print(f"Error inserting row into database: {e}")
                continue

    conn.commit()
    conn.close()

    print(f"Updated database with {total_new_rows} new rows.")

def update_db_to_latest():
    """
    Look up the maximum timestamp in glotec.db, then call update_db()
    with that timestamp so that only entries later than that are fetched.
    """
    db_filename = "glotec.db"
    try:
        conn = sqlite3.connect(db_filename)
        cur = conn.cursor()
        # find max timestamp
        cur.execute('SELECT MAX(timestamp) FROM glotec;')
        row = cur.fetchone()
        conn.close()
    except Exception as e:
        print(f"Error opening or querying {db_filename}: {e}")
        sys.exit(1)

    if not row or row[0] is None:
        print("No existing timestamp found in glotec.db; use -alldb or -updatedb instead.")
        sys.exit(1)

    latest_timestamp = row[0]
    print(f"Latest timestamp in database is {latest_timestamp}. Updating from that point forward.")
    # now call the existing update_db function
    update_db(latest_timestamp, latest_timestamp)

# -------------------------
# Function for updating the database from a netCDF4 file.
def update_db_from_ncd(netcdf_link):
    """
    Accepts a URL to a netCDF4 file. The netCDF file is expected to contain variables:
      - time (shape: 144)
      - latitude (shape: 72)
      - longitude (shape: 72)
      - hmF2 (shape: (144, 72, 72))
      - quality_flag (shape: (144, 72, 72))
    where the hmF2 and quality_flag variables have dimensions ('time', 'latitude', 'longitude').
    The function downloads the netCDF file, loops through each time index and each grid cell,
    creates a record with columns:
       timestamp, longitude, latitude, hmF2, NmF2, quality_flag,
    and appends each record to the existing glotec.db database.
    For netCDF files (which do not contain a NmF2 variable), NmF2 is stored as NULL.
    """
    print(f"Updating database from netCDF file: {netcdf_link}")
    try:
        r = requests.get(netcdf_link)
        r.raise_for_status()
    except Exception as e:
        print(f"Error downloading netCDF file from {netcdf_link}: {e}")
        sys.exit(1)

    # Write the downloaded netCDF content to a temporary file.
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        tmp.write(r.content)
        tmp.flush()
        tmp_filename = tmp.name

    try:
        ds = Dataset(tmp_filename, 'r')
    except Exception as e:
        print(f"Error opening netCDF file: {e}")
        sys.exit(1)

    try:
        time_var = ds.variables["time"][:]      # shape (144,)
        lat_var = ds.variables["latitude"][:]     # shape (72,)
        lon_var = ds.variables["longitude"][:]      # shape (72,)
        hmF2_var = ds.variables["hmF2"][:]          # shape (144, 72, 72)
        qflag_var = ds.variables["quality_flag"][:]  # shape (144, 72, 72)
        # Convert the time variable to datetime objects using its units.
        time_units = ds.variables["time"].units
        time_calendar = ds.variables["time"].calendar if "calendar" in ds.variables["time"].ncattrs() else "standard"
        times = num2date(time_var, units=time_units, calendar=time_calendar)
    except Exception as e:
        print(f"Error retrieving variables from netCDF: {e}")
        ds.close()
        sys.exit(1)

    ds.close()

    db_filename = "glotec.db"
    try:
        conn = sqlite3.connect(db_filename)
        cur = conn.cursor()
    except Exception as e:
        print(f"Error opening SQLite database {db_filename}: {e}")
        sys.exit(1)

    total_new_rows = 0
    n_time = len(times)
    n_lat = len(lat_var)
    n_lon = len(lon_var)

    print(f"Processing netCDF data with {n_time} time steps, {n_lat} latitudes, {n_lon} longitudes.")
    for t in range(n_time):
        timestamp_str = times[t].strftime("%Y-%m-%dT%H:%M:%SZ")
        for i in range(n_lat):
            for j in range(n_lon):
                lat_val = float(lat_var[i])
                lon_val = float(lon_var[j])
                hmF2_val = hmF2_var[t, i, j]
                # Replace any masked element with nan.
                hmF2_val_filled = np.ma.filled(hmF2_val, np.nan)
                if np.isnan(hmF2_val_filled):
                    hmF2_int = None
                else:
                    hmF2_int = math.trunc(float(hmF2_val_filled))
                quality_flag = str(qflag_var[t, i, j])
                # For netCDF updates, there is no NmF2 variable; store as NULL (None).
                NmF2_val = None
                try:
                    cur.execute("""
                        INSERT INTO glotec (timestamp, longitude, latitude, hmF2, NmF2, quality_flag)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (timestamp_str, lon_val, lat_val, hmF2_int, NmF2_val, quality_flag))
                    total_new_rows += 1
                except Exception as e:
                    print(f"Error inserting record for time {timestamp_str}, lat {lat_val}, lon {lon_val}: {e}")
                    continue

    conn.commit()
    conn.close()

    print(f"Updated database from netCDF file: {total_new_rows} new rows inserted.")


# -------------------------
# New function for NM patch.
def nm_patch(start_timestamp, end_timestamp):
    """
    Accepts two timestamp strings in ISO 8601 format (e.g., 2025-02-03T00:45:00Z and 2025-02-03T01:00:00Z).
    This function will:
      1. Delete all rows in the glotec table whose timestamp is between start_timestamp and end_timestamp (inclusive).
      2. Fetch the NOAA listing from LISTING_URL, filter for entries whose time_tag falls within that range,
         and process each such file (including retrieving NmF2 values) to reinsert the corresponding data.
    """
    try:
        start_dt = datetime.strptime(start_timestamp, "%Y-%m-%dT%H:%M:%SZ")
        end_dt = datetime.strptime(end_timestamp, "%Y-%m-%dT%H:%M:%SZ")
    except Exception as e:
        print(f"Error parsing provided timestamps: {e}")
        sys.exit(1)

    db_filename = "glotec.db"
    try:
        conn = sqlite3.connect(db_filename)
        cur = conn.cursor()
    except Exception as e:
        print(f"Error opening SQLite database {db_filename}: {e}")
        sys.exit(1)

    # Delete rows with timestamp between start_timestamp and end_timestamp.
    try:
        cur.execute("DELETE FROM glotec WHERE timestamp >= ? AND timestamp <= ?;", (start_timestamp, end_timestamp))
        deleted = cur.rowcount
        conn.commit()
        print(f"Deleted {deleted} rows from glotec between {start_timestamp} and {end_timestamp}.")
    except Exception as e:
        print(f"Error deleting rows: {e}")
        conn.close()
        sys.exit(1)

    # Now fetch the listing and filter for entries within the date range.
    try:
        r = requests.get(LISTING_URL)
        r.raise_for_status()
        full_listing = r.json()
    except Exception as e:
        print(f"Error downloading listing JSON: {e}")
        conn.close()
        sys.exit(1)

    filtered_entries = []
    for entry in full_listing:
        try:
            file_dt = datetime.strptime(entry["time_tag"], "%Y-%m-%dT%H:%M:%SZ")
        except Exception as e:
            print(f"Error parsing time_tag {entry.get('time_tag')}: {e}")
            continue

        if start_dt <= file_dt <= end_dt:
            filtered_entries.append(entry)

    if not filtered_entries:
        print("No entries found in the listing within the provided date range.")
        conn.close()
        sys.exit(0)

    print(f"Found {len(filtered_entries)} entries to patch.")

    total_new_rows = 0

    for entry in filtered_entries:
        timestamp_str = entry["time_tag"]

        rel_url = entry.get("url")
        if not rel_url:
            print("Skipping an entry because it has no 'url' field.")
            continue

        full_url = BASE_URL + rel_url
        print("Processing file for NM patch: " + full_url)

        try:
            r = requests.get(full_url)
            r.raise_for_status()
            geojson_data = r.json()
        except Exception as e:
            print(f"Error downloading GeoJSON from {full_url}: {e}")
            continue

        features = geojson_data.get("features", [])
        for i, feature in enumerate(features):
            properties = feature.get("properties", {})
            geometry = feature.get("geometry", {})

            if geometry.get("type") != "Point":
                print(f"Skipping feature {i} in file {rel_url}: geometry type is not 'Point'.")
                continue

            coords = geometry.get("coordinates", [])
            if len(coords) < 2:
                print(f"Skipping feature {i} in file {rel_url}: insufficient coordinate data.")
                continue

            longitude, latitude = coords[0], coords[1]

            hmF2 = properties.get("hmF2")
            NmF2 = properties.get("NmF2")
            quality_flag = properties.get("quality_flag")

            try:
                hmF2 = math.trunc(float(hmF2))
            except Exception:
                print("hmF2 failed " + str(hmF2))
                hmF2 = None

            try:
                NmF2 = float(NmF2) if NmF2 is not None else None
            except Exception:
                print("NmF2 conversion failed " + str(NmF2))
                NmF2 = None

            try:
                cur.execute("""
                    INSERT INTO glotec (timestamp, longitude, latitude, hmF2, NmF2, quality_flag)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (timestamp_str, longitude, latitude, hmF2, NmF2, quality_flag))
                total_new_rows += 1
            except Exception as e:
                print(f"Error inserting row into database: {e}")
                continue

    conn.commit()
    conn.close()

    print(f"NM patch complete: {total_new_rows} new rows inserted for entries between {start_timestamp} and {end_timestamp}.")


def outing_glotec(csv_url):
    """
    Reads a CSV-like file from csv_url where QSOs start at line 5 (1-indexed),
    with lines formatted like: callsign,YYYY/MM/DD HH:MM:SS,RSTsent,RSTrcvd

    It computes:
      begin = 10 minutes before first QSO timestamp
      end   = 10 minutes after last  QSO timestamp

    and then invokes update_db(begin, end).
    """
    try:
        r = requests.get(csv_url)
        r.raise_for_status()
        text = r.text
    except Exception as e:
        print(f"Error downloading QSO file from {csv_url}: {e}")
        sys.exit(1)

    lines = text.splitlines()
    if len(lines) < 5:
        print("File has fewer than 5 lines; no QSO lines found.")
        sys.exit(1)

    # QSOs start at line 5 (index 4)
    qso_lines = [ln.strip() for ln in lines[4:] if ln.strip()]

    first_dt = None
    last_dt = None

    for ln in qso_lines:
        # Expect at least "call,timestamp,..."
        parts = [p.strip() for p in ln.split(",")]
        if len(parts) < 2:
            continue
        ts_str = parts[1]
        try:
            # Interpret QSO timestamps as UTC
            dt = datetime.strptime(ts_str, "%Y/%m/%d %H:%M:%S")
        except ValueError:
            # Skip non-QSO or malformed lines
            continue

        if first_dt is None or dt < first_dt:
            first_dt = dt
        if last_dt is None or dt > last_dt:
            last_dt = dt

    if first_dt is None or last_dt is None:
        print("No valid QSO timestamps found.")
        sys.exit(1)

    begin_dt = first_dt - timedelta(minutes=10)
    end_dt   = last_dt + timedelta(minutes=10)

    begin_iso = begin_dt.strftime("%Y-%m-%dT%H:%M:%SZ")
    end_iso   = end_dt.strftime("%Y-%m-%dT%H:%M:%SZ")

    print(f"outing_glotec → computed time window:")
    print(f"  first QSO: {first_dt}  last QSO: {last_dt}")
    print(f"  begin:     {begin_iso}")
    print(f"  end:       {end_iso}")

    # Equivalent to calling: glotec.py -updatedb <begin_iso> <end_iso>
    update_db(begin_iso, end_iso)


# -------------------------
# New function for -laglotec mode.
def latest_glotec():
    """
    Retrieves only the most recent glotec entry from the listing and creates a new glotec database
    containing only the records from that single entry.
    """
    print("Running in -laglotec mode: creating new glotec database from the most recent entry.")
    try:
        r = requests.get(LISTING_URL)
        r.raise_for_status()
        listing = r.json()
    except Exception as e:
        print(f"Error downloading listing JSON: {e}")
        sys.exit(1)

    if not listing:
        print("No entries found in the listing.")
        sys.exit(1)

    # Find the entry with the maximum (most recent) time_tag.
    try:
        for entry in listing:
            entry["time_dt"] = datetime.strptime(entry["time_tag"], "%Y-%m-%dT%H:%M:%SZ")
    except Exception as e:
        print(f"Error parsing time_tag in listing: {e}")
        sys.exit(1)

    latest_entry = max(listing, key=lambda e: e["time_dt"])
    print(f"Most recent entry found: {latest_entry['time_tag']}")

    # Create (or overwrite) the glotec database.
    db_filename = "glotec.db"
    try:
        conn = sqlite3.connect(db_filename)
        cur = conn.cursor()
        cur.execute("DROP TABLE IF EXISTS glotec;")
        cur.execute("""
            CREATE TABLE glotec (
                uid INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                longitude REAL,
                latitude REAL,
                hmF2 INTEGER,
                NmF2 REAL,
                quality_flag TEXT
            )
        """)
        conn.commit()
    except Exception as e:
        print(f"Error setting up new SQLite database {db_filename}: {e}")
        sys.exit(1)

    # Process only the latest entry.
    timestamp_str = latest_entry["time_tag"]
    rel_url = latest_entry.get("url")
    if not rel_url:
        print("The most recent entry has no 'url' field.")
        conn.close()
        sys.exit(1)
    full_url = BASE_URL + rel_url
    print("Processing most recent file: " + full_url)

    try:
        r = requests.get(full_url)
        r.raise_for_status()
        geojson_data = r.json()
    except Exception as e:
        print(f"Error downloading GeoJSON from {full_url}: {e}")
        conn.close()
        sys.exit(1)

    total_rows = 0
    features = geojson_data.get("features", [])
    for i, feature in enumerate(features):
        properties = feature.get("properties", {})
        geometry = feature.get("geometry", {})

        if geometry.get("type") != "Point":
            print(f"Skipping feature {i} in file {rel_url}: geometry type is not 'Point'.")
            continue

        coords = geometry.get("coordinates", [])
        if len(coords) < 2:
            print(f"Skipping feature {i} in file {rel_url}: insufficient coordinate data.")
            continue

        longitude, latitude = coords[0], coords[1]

        hmF2 = properties.get("hmF2")
        NmF2 = properties.get("NmF2")
        quality_flag = properties.get("quality_flag")

        try:
            hmF2 = math.trunc(float(hmF2))
        except Exception:
            print("hmF2 failed " + str(hmF2))
            hmF2 = None

        try:
            NmF2 = float(NmF2) if NmF2 is not None else None
        except Exception:
            print("NmF2 conversion failed " + str(NmF2))
            NmF2 = None

        try:
            cur.execute("""
                INSERT INTO glotec (timestamp, longitude, latitude, hmF2, NmF2, quality_flag)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (timestamp_str, longitude, latitude, hmF2, NmF2, quality_flag))
            total_rows += 1
        except Exception as e:
            print(f"Error inserting row into database: {e}")
            continue

    conn.commit()
    conn.close()
    print(f"Created new glotec database with {total_rows} rows from the most recent entry.")

def slice_databases(start_timestamp, end_timestamp):
    """
    Creates new database files that contain all rows from the original databases between 
    and including start_timestamp and end_timestamp. The new files are:
      - glotec_slice.db, which will have the same table name "glotec" as the original glotec.db.
      - rm_toucans_slice.db, which will have the same table name "rm_rnb_history_pres" as the original rm_toucans.db.
    """
    import sqlite3

    # Slice glotec.db -> glotec_slice.db
    in_db = "glotec.db"
    out_db = "glotec_slice.db"
    try:
        conn_in = sqlite3.connect(in_db)
        cur_in = conn_in.cursor()
        cur_in.execute("SELECT sql FROM sqlite_master WHERE type='table' AND name='glotec'")
        schema_row = cur_in.fetchone()
        if schema_row is None:
            print(f"Error: Table 'glotec' not found in {in_db}.")
            sys.exit(1)
        create_sql = schema_row[0]
    except Exception as e:
        print(f"Error reading schema from {in_db}: {e}")
        sys.exit(1)
    try:
        conn_out = sqlite3.connect(out_db)
        cur_out = conn_out.cursor()
        cur_out.execute("DROP TABLE IF EXISTS glotec")
        cur_out.execute(create_sql)
        # Copy rows between the timestamps
        cur_in.execute("SELECT * FROM glotec WHERE timestamp >= ? AND timestamp <= ?", (start_timestamp, end_timestamp))
        rows = cur_in.fetchall()
        for row in rows:
            placeholders = ",".join(["?"] * len(row))
            cur_out.execute("INSERT INTO glotec VALUES (" + placeholders + ")", row)
        conn_out.commit()
        conn_out.close()
        conn_in.close()
        print(f"Sliced {len(rows)} rows from {in_db} into {out_db}.")
    except Exception as e:
        print("Error slicing glotec database:", e)
        sys.exit(1)

    # Slice QSO database.
    # We assume the original QSO database is in rm_toucans.db with table rm_rnb_history_pres.
    in_db_qso = "rm_toucans.db"
    # Changed output file name to rm_toucans_slice.db to preserve original name inside.
    out_db_qso = "rm_toucans_slice.db"
    try:
        conn_in_qso = sqlite3.connect(in_db_qso)
        cur_in_qso = conn_in_qso.cursor()
        cur_in_qso.execute("SELECT sql FROM sqlite_master WHERE type='table' AND name='rm_rnb_history_pres'")
        schema_row_qso = cur_in_qso.fetchone()
        if schema_row_qso is None:
            print(f"Error: Table 'rm_rnb_history_pres' not found in {in_db_qso}.")
            sys.exit(1)
        create_sql_qso = schema_row_qso[0]
    except Exception as e:
        print(f"Error reading schema from {in_db_qso}: {e}")
        sys.exit(1)
    try:
        conn_out_qso = sqlite3.connect(out_db_qso)
        cur_out_qso = conn_out_qso.cursor()
        cur_out_qso.execute("DROP TABLE IF EXISTS rm_rnb_history_pres")
        cur_out_qso.execute(create_sql_qso)
        # Copy rows between the timestamps.
        cur_in_qso.execute("SELECT * FROM rm_rnb_history_pres WHERE timestamp >= ? AND timestamp <= ?", (start_timestamp, end_timestamp))
        rows_qso = cur_in_qso.fetchall()
        for row in rows_qso:
            placeholders = ",".join(["?"] * len(row))
            cur_out_qso.execute("INSERT INTO rm_rnb_history_pres VALUES (" + placeholders + ")", row)
        conn_out_qso.commit()
        conn_out_qso.close()
        conn_in_qso.close()
        print(f"Sliced {len(rows_qso)} rows from {in_db_qso} into {out_db_qso}.")
    except Exception as e:
        print("Error slicing QSO database:", e)
        sys.exit(1)

# -------------------------
# Main script execution.
if "-alldb" in sys.argv:
    dump_all_data()
    sys.exit(0)

if "-outing_glotec" in sys.argv:
    try:
        idx = sys.argv.index("-outing_glotec")
        csv_url_arg = sys.argv[idx + 1]
    except IndexError:
        print("Error: -outing_glotec requires a URL to the QSO CSV file.")
        sys.exit(1)
    outing_glotec(csv_url_arg)
    sys.exit(0)


if "-slice" in sys.argv:
    try:
        idx = sys.argv.index("-slice")
        start_slice = sys.argv[idx + 1]
        end_slice = sys.argv[idx + 2]
    except IndexError:
        print("Error: -slice option requires two timestamp arguments (e.g., 2025-03-10T00:00:00Z 2025-03-10T23:59:59Z)")
        sys.exit(1)
    slice_databases(start_slice, end_slice)
    sys.exit(0)

if "-updatedb" in sys.argv:
    try:
        idx = sys.argv.index("-updatedb")
        timestamp_arg = sys.argv[idx + 1]
    except IndexError:
        print("Error: -updatedb option requires a timestamp argument formatted as 2025-02-03T00:45:00Z")
        sys.exit(1)
    try:
        timestamp_end_arg = sys.argv[idx+2]
    except IndexError:
        timestamp_end_arg = timestamp_arg
    update_db(timestamp_arg, timestamp_end_arg)
    sys.exit(0)

if "-updatelatest" in sys.argv:
    update_db_to_latest()
    sys.exit(0)

if "-appnd_ncd" in sys.argv:
    try:
        idx = sys.argv.index("-appnd_ncd")
        netcdf_link = sys.argv[idx + 1]
    except IndexError:
        print("Error: -appnd_ncd option requires a netCDF4 file URL as an argument.")
        sys.exit(1)
    update_db_from_ncd(netcdf_link)
    sys.exit(0)

if "-nmpatch" in sys.argv:
    try:
        idx = sys.argv.index("-nmpatch")
        start_patch = sys.argv[idx + 1]
        end_patch = sys.argv[idx + 2]
    except IndexError:
        print("Error: -nmpatch option requires two timestamp arguments formatted as 2025-02-03T00:45:00Z")
        sys.exit(1)
    nm_patch(start_patch, end_patch)
    sys.exit(0)

if "-laglotec" in sys.argv:
    latest_glotec()
    sys.exit(0)

# --- Normal CZML mode below ---
try:
    r = requests.get(LISTING_URL)
    r.raise_for_status()
    listing = r.json()
except Exception as e:
    print(f"Error downloading listing JSON: {e}")
    sys.exit(1)

if not listing:
    print("No entries found in the listing.")
    sys.exit(1)

for entry in listing:
    try:
        entry["time_dt"] = datetime.strptime(entry["time_tag"], "%Y-%m-%dT%H:%M:%SZ")
    except Exception as e:
        print(f"Error parsing time_tag {entry.get('time_tag')} : {e}")
        sys.exit(1)

sorted_listing = sorted(listing, key=lambda e: e["time_dt"])
closest_entry = min(sorted_listing, key=lambda e: abs(e["time_dt"] - target_dt))
closest_index = sorted_listing.index(closest_entry)

print(f"Target datetime: {target_dt}")
print(f"Closest file time_tag: {closest_entry['time_tag']}")

entries = sorted_listing[closest_index:closest_index + NUM_URLS]
if len(entries) == 0:
    print("No entries available after the target time.")
    sys.exit(1)

interval_delta = timedelta(minutes=10)
start_times = [entry["time_dt"] for entry in entries]
end_times = [entry["time_dt"] + interval_delta for entry in entries]

overall_start = min(start_times)
overall_end = max(end_times)

clock_interval = f"{overall_start.strftime('%Y-%m-%dT%H:%M:%SZ')}/{overall_end.strftime('%Y-%m-%dT%H:%M:%SZ')}"

color_scale = [
    {"rgba": [0, 0, 0, 255]},
    {"rgba": [165, 42, 42, 255]},
    {"rgba": [255, 0, 0, 255]},
    {"rgba": [255, 165, 0, 255]},
    {"rgba": [255, 255, 0, 255]},
    {"rgba": [0, 128, 0, 255]},
    {"rgba": [0, 0, 255, 255]},
    {"rgba": [238, 130, 238, 255]},
    {"rgba": [255, 255, 255, 255]}
]
n_colors = len(color_scale)

min_alt_km = 150.0
max_alt_km = 500.0
range_alt = max_alt_km - min_alt_km

def get_color_for_altitude(hmF2_km):
    if hmF2_km < min_alt_km:
        hmF2_km = min_alt_km
    elif hmF2_km > max_alt_km:
        hmF2_km = max_alt_km

    normalized = (hmF2_km - min_alt_km) / range_alt
    idx = int(normalized * (n_colors - 1))
    return color_scale[idx]

czml = []

document_packet = {
    "id": "document",
    "name": "Ionospheric hmF2 Paths Animated",
    "version": "1.0",
    "clock": {
        "interval": clock_interval,
        "currentTime": overall_start.strftime('%Y-%m-%dT%H:%M:%SZ')
    }
}
czml.append(document_packet)

for entry in entries:
    rel_url = entry.get("url")
    if not rel_url:
        print("Skipping an entry because it has no 'url' field.")
        continue
    full_url = BASE_URL + rel_url
    print("processing " + full_url)

    start_time = entry["time_dt"]
    end_time = start_time + interval_delta
    show_interval = f"{start_time.strftime('%Y-%m-%dT%H:%M:%SZ')}/{end_time.strftime('%Y-%m-%dT%H:%M:%SZ')}"

    try:
        r = requests.get(full_url)
        r.raise_for_status()
        geojson_data = r.json()
    except Exception as e:
        print(f"Error downloading GeoJSON from {full_url}: {e}")
        continue

    features = geojson_data.get("features", [])
    for i, feature in enumerate(features):
        properties = feature.get("properties", {})
        geometry = feature.get("geometry", {})

        if geometry.get("type") != "Point":
            print(f"Skipping feature {i} in file {rel_url}: geometry type is not 'Point'.")
            continue

        coords = geometry.get("coordinates", [])
        if len(coords) < 2:
            print(f"Skipping feature {i} in file {rel_url}: insufficient coordinate data.")
            continue

        lon, lat = coords[0], coords[1]
        hmF2 = properties.get("hmF2")
        if hmF2 is None:
            print(f"Skipping feature {i} in file {rel_url}: no hmF2 value found.")
            continue
        try:
            hmF2 = float(hmF2)
        except ValueError:
            print(f"Skipping feature {i} in file {rel_url}: hmF2 value is not numeric.")
            continue

        alt_m = hmF2 * 1000.0
        color = get_color_for_altitude(hmF2)
        positions = [lon, lat, 0, lon, lat, alt_m]
        entity_id = f"{entry['time_tag']}_feature_{i}"
        entity = {
            "id": entity_id,
            "name": f"hmF2: {hmF2} km, time: {entry['time_tag']}",
            "polyline": {
                "positions": {
                    "cartographicDegrees": positions
                },
                "material": {
                    "solidColor": {
                        "color": color
                    }
                },
                "width": 2,
                "show": [
                    {
                        "interval": show_interval,
                        "boolean": True
                    }
                ]
            }
        }
        czml.append(entity)

output_filename = "animated_output.czml"
try:
    with open(output_filename, "w") as f:
        json.dump(czml, f, indent=2)
    print(f"CZML file successfully written to '{output_filename}'.")
except Exception as e:
    print(f"Error writing CZML file: {e}")
